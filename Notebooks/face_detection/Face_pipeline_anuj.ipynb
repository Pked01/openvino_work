{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:39.033523Z",
     "start_time": "2019-09-10T07:14:39.027709Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser, SUPPRESS\n",
    "import cv2,threading\n",
    "import time\n",
    "import logging as log\n",
    "import numpy as np\n",
    "from multiprocessing import Process,Pool\n",
    "import shutil,pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from openvino.inference_engine import IEPlugin,IENetwork\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:39.225692Z",
     "start_time": "2019-09-10T07:14:39.217358Z"
    }
   },
   "outputs": [],
   "source": [
    "m_fd = '../model_downloader/Retail/object_detection/face/sqnet1.0modif-ssd/0004/dldt/face-detection-retail-0004.xml'\n",
    "m_ag = '../model_downloader/Retail/object_attributes/age_gender/dldt/age-gender-recognition-retail-0013.xml'\n",
    "m_hp = '../model_downloader/Transportation/object_attributes/headpose/vanilla_cnn/dldt/head-pose-estimation-adas-0001.xml'\n",
    "m_em = '../model_downloader/Retail/object_attributes/emotions_recognition/0003/dldt/emotions-recognition-retail-0003.xml'\n",
    "# m_freid = '../model_downloader/Retail/object_reidentification/face/mobilenet_based/dldt/face-reidentification-retail-0095.xml'\n",
    "channel = 'rtsp://admin:admin@123@10.10.12.14:554/Streaming/Channels/401/'\n",
    "cpu_extension = '../build_samples/intel64/Release/lib/libcpu_extension.so'\n",
    "device = 'CPU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:39.459488Z",
     "start_time": "2019-09-10T07:14:39.425230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plugin initialization for specified device and load extensions library if specified\n",
    "log.info(\"Initializing plugin for {} device...\".format('CPU'))\n",
    "plugin = IEPlugin(device='CPU')\n",
    "if cpu_extension and 'CPU' in device:\n",
    "    plugin.add_cpu_extension(cpu_extension)\n",
    "# Read IR\n",
    "log.info(\"Reading IR...\")\n",
    "m_fd_net = IENetwork(model=m_fd, weights=os.path.splitext(m_fd)[0] + \".bin\")\n",
    "m_ag_net = IENetwork(model=m_ag, weights=os.path.splitext(m_ag)[0] + \".bin\")\n",
    "m_hp_net = IENetwork(model=m_hp, weights=os.path.splitext(m_hp)[0] + \".bin\")\n",
    "m_em_net = IENetwork(model=m_em, weights=os.path.splitext(m_em)[0] + \".bin\")\n",
    "# m_freid_net = IENetwork(model=m_freid, weights=os.path.splitext(m_freid)[0] + \".bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:39.578232Z",
     "start_time": "2019-09-10T07:14:39.567214Z"
    }
   },
   "outputs": [],
   "source": [
    "if device == \"CPU\":\n",
    "#     for net in [m_fd_net,m_ag_net,m_hp_net,m_em_net,m_freid_net]:\n",
    "    for net in [m_fd_net,m_ag_net,m_hp_net,m_em_net]:\n",
    "        supported_layers = plugin.get_supported_layers(net)\n",
    "        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "        if len(not_supported_layers) != 0:\n",
    "            log.error(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                      format(plugin.device, ', '.join(not_supported_layers)))\n",
    "            log.error(\"Please try to specify cpu extensions library path in demo's command line parameters using -l \"\n",
    "                      \"or --cpu_extension command line argument\")\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:40.024067Z",
     "start_time": "2019-09-10T07:14:39.748994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 300 3 300\n",
      "1 62 3 62\n",
      "1 60 3 60\n",
      "1 64 3 64\n"
     ]
    }
   ],
   "source": [
    "input_blob = []\n",
    "out_blob = []\n",
    "exec_net = []\n",
    "# for i,net in enumerate([m_fd_net,m_ag_net,m_hp_net,m_em_net,m_freid_net]):\n",
    "for i,net in enumerate([m_fd_net,m_ag_net,m_hp_net,m_em_net]):\n",
    "    # assert len(net.inputs.keys()) == 1, \"Demo supports only single input topologies\"\n",
    "    # assert len(net.outputs) == 1, \"Demo supports only single output topologies\"\n",
    "    input_blob.append(next(iter(net.inputs)))\n",
    "    out_blob.append(next(iter(net.outputs)))\n",
    "    log.info(\"Loading IR to the plugin...\")\n",
    "    exec_net.append(plugin.load(network=net, num_requests=16))\n",
    "    # Read and pre-process input image\n",
    "    n, c, h, w = net.inputs[input_blob[i]].shape\n",
    "    print(n,h,c,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:40.051960Z",
     "start_time": "2019-09-10T07:14:40.025606Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_fd(input_frame,n=1,c=3,w=300,h=300,thresh=.1):\n",
    "    op_frame = cv2.resize(input_frame,(w,h)).transpose((2, 0, 1)).reshape(n,c,h,w) \n",
    "    ### we can add multiple requests and just enumerate request ids\n",
    "    exec_net[0].start_async(request_id=1, inputs={input_blob[0]: op_frame})\n",
    "    if exec_net[0].requests[1].wait(-1)==0:\n",
    "        res = exec_net[0].requests[1].outputs[out_blob[0]]\n",
    "    res_filt =  res[np.where(res[:,:,:,2]>thresh)]\n",
    "    res_filt = res_filt[np.min(res_filt,axis=1)>=0]\n",
    "    return res_filt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:40.163821Z",
     "start_time": "2019-09-10T07:14:40.106606Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_ag(input_frame,bboxes,n=1,c=3,w=62,h=62):\n",
    "    \"\"\"\n",
    "    output : age/100\n",
    "    prob : [female, male]\n",
    "    \"\"\"\n",
    "    \n",
    "    res = []\n",
    "    faces = [cv2.resize(input_frame[b[1]:b[3],b[0]:b[2]],(w,h)).transpose((2, 0, 1)).reshape(n,c,h,w) for b in bboxes]\n",
    "    ### we can add multiple requests and just enumerate request ids\n",
    "    [exec_net[1].start_async(request_id=cursor_id, inputs={input_blob[1]: face}) for cursor_id,face in enumerate(faces)]\n",
    "    for i in range(len(faces)):\n",
    "        if exec_net[1]. requests[i].wait(-1)==0:\n",
    "            res.append(exec_net[1].requests[i].outputs)\n",
    "    age = [int(i['age_conv3']*100) for i in res]\n",
    "    gender = [ 'Female' if i['prob'][0][0]>i['prob'][0][1] else 'Male' for i in res]\n",
    "    return list(zip(age,gender))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:40.289110Z",
     "start_time": "2019-09-10T07:14:40.281524Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_hp(input_frame,bboxes,n=1,c=3,w=60,h=60):\n",
    "    res = []\n",
    "    faces = [cv2.resize(input_frame[b[1]:b[3],b[0]:b[2]],(w,h)).transpose((2, 0, 1)).reshape(n,c,h,w) for b in bboxes]\n",
    "    ### we can add multiple requests and just enumerate request ids\n",
    "    [exec_net[2].start_async(request_id=cursor_id, inputs={input_blob[2]: face}) for cursor_id,face in enumerate(faces)]\n",
    "    for i in range(len(faces)):\n",
    "        if exec_net[2].requests[i].wait(-1)==0:\n",
    "            res.append(exec_net[2].requests[i].outputs)\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:40.518997Z",
     "start_time": "2019-09-10T07:14:40.506007Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_em(input_frame,bboxes,n=1,c=3,w=64,h=64):\n",
    "    \"\"\"\n",
    "    'neutral', 'happy', 'sad', 'surprise', 'anger'\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    faces = [cv2.resize(input_frame[b[1]:b[3],b[0]:b[2]],(w,h)).transpose((2, 0, 1)).reshape(n,c,h,w) for b in bboxes]\n",
    "    ### we can add multiple requests and just enumerate request ids\n",
    "    [exec_net[3].start_async(request_id=cursor_id, inputs={input_blob[3]: face}) for cursor_id,face in enumerate(faces)]\n",
    "    for i in range(len(faces)):\n",
    "        if exec_net[3].requests[i].wait(-1)==0:\n",
    "            res.append(exec_net[3].requests[i].outputs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:40.715301Z",
     "start_time": "2019-09-10T07:14:40.703693Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_face_embedding(input_frame,bboxes,n=1,c=3,w=128,h=128):\n",
    "    res = []\n",
    "    faces = [cv2.resize(input_frame[b[1]:b[3],b[0]:b[2]],(w,h)).transpose((2, 0, 1)).reshape(n,c,h,w) for b in bboxes]\n",
    "    ### we can add multiple requests and just enumerate request ids\n",
    "    [exec_net[4].start_async(request_id=cursor_id, inputs={input_blob[4]: face}) for cursor_id,face in enumerate(faces)]\n",
    "    for i in range(len(faces)):\n",
    "        if exec_net[4].requests[i].wait(-1)==0:\n",
    "            res.append(exec_net[4].requests[i].outputs)\n",
    "    return np.array([i['658'].flatten() for i in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:41.041359Z",
     "start_time": "2019-09-10T07:14:41.032855Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_file_path(source_dir,ext=('.jpg','.png','.jpeg')):\n",
    "    \"\"\"\n",
    "    all images with csv extension exist in set of dirs\n",
    "    \"\"\"\n",
    "    op =[]\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(ext):\n",
    "                \n",
    "                 op.append(os.path.join(os.path.abspath(root), file))\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:41.543795Z",
     "start_time": "2019-09-10T07:14:41.528643Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_face_detection(train_data_location,thresh=.5):\n",
    "    \"\"\"\n",
    "    train_data is in format of person name as folder name and containing images\n",
    "    this function will return classifier and label_encoder\n",
    "    \n",
    "    \"\"\"\n",
    "    imgs = get_all_file_path(train_data_location)\n",
    "    labels = [os.path.basename(os.path.dirname(f)) for f in imgs]\n",
    "    embeddings = []\n",
    "    for f in imgs:\n",
    "        im = cv2.imread(f)\n",
    "        initial_h,initial_w = im.shape[:2]\n",
    "        while True:\n",
    "            res_filt = generate_fd(im,thresh=thresh)\n",
    "            bboxes = np.multiply([[initial_w,initial_h,initial_w,initial_h]],(res_filt[:,3:])).astype('int')\n",
    "            embedding = generate_face_embedding(im,bboxes)\n",
    "            if embedding.shape[0]>1:\n",
    "                thresh = thresh+.05\n",
    "            elif embedding.shape[0]==0:\n",
    "                thresh = thresh-.05\n",
    "            else:\n",
    "                embeddings.append(embedding)\n",
    "                break\n",
    "    embedding_array = np.concatenate(embeddings)\n",
    "    L_enc = LabelEncoder()\n",
    "    labels_enc = L_enc.fit_transform(labels)\n",
    "#     print(len(embeddings),embedding_array.shape,labels_enc.shape)\n",
    "    clf = LogisticRegression(n_jobs=-1,class_weight='balanced')\n",
    "    clf.fit(embedding_array,labels_enc)\n",
    "    pickle.dump(L_enc,open(os.path.join(train_data_location,'label_encoder.pickle'),'wb'))\n",
    "    pickle.dump(clf,open(os.path.join(train_data_location,'classifier.pickle'),'wb'))\n",
    "    return clf, L_enc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:41.925675Z",
     "start_time": "2019-09-10T07:14:41.914376Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_plot(in_frame,clf,L_enc,thresh = .2):\n",
    "    \"\"\"\n",
    "    input_frame\n",
    "    clf : classfier for face recognition\n",
    "    L_enc : label_encoder for face recognition\n",
    "    initial_h: initial_height of frame\n",
    "    initial_w : initial_width of frame\n",
    "    \"\"\"\n",
    "    ### all detect and plot should be called sequently \n",
    "    initial_h,initial_w = in_frame.shape[:2]\n",
    "    res_filt = generate_fd(frame,thresh=thresh)\n",
    "    bboxes = np.multiply([[initial_w,initial_h,initial_w,initial_h]],(res_filt[:,3:])).astype('int')\n",
    "    for b in bboxes:\n",
    "        # in_frame[b[1]:b[3],b[0]:b[2]]\n",
    "        cv2.rectangle(in_frame, (b[0], b[1]), (b[2], b[3]), (255, 255, 255), 2)\n",
    "        \n",
    "#     if len(bboxes)>0:\n",
    "#         names = L_enc.inverse_transform(clf.predict(generate_face_embedding(frame,bboxes)))\n",
    "#         age_gender = generate_ag(frame,bboxes)\n",
    "#         for name,a_g,b in zip(names,age_gender,bboxes):\n",
    "#             if a_g[1]=='Female':\n",
    "#                 pink = (193,182,255)\n",
    "#                 deep_pink = (193,20,255)\n",
    "#                 caption = name.upper()+'('+str(a_g[0]) + ' , '+a_g[1]+')'\n",
    "#                 cv2.rectangle(in_frame, (b[0], b[1]), (b[2], b[3]), pink, 2)\n",
    "#                 cv2.putText(frame, caption,\n",
    "#                                     (b[0]-10, b[1] - 10),cv2.FONT_HERSHEY_COMPLEX, 0.6, deep_pink, 1)\n",
    "#             else:\n",
    "#                 blue = (255,0,0)\n",
    "#                 deep_blue = (200,0,0)\n",
    "#                 caption = name.upper()+'('+str(a_g[0]) + ' , '+a_g[1]+')'\n",
    "#                 cv2.rectangle(in_frame, (b[0], b[1]), (b[2], b[3]), blue, 2)\n",
    "#                 cv2.putText(frame, caption,\n",
    "#                                     (b[0]-10, b[1] - 10),cv2.FONT_HERSHEY_COMPLEX, 0.6, deep_blue, 1)\n",
    "#     return frame\n",
    "    \n",
    "    return in_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:14:43.149181Z",
     "start_time": "2019-09-10T07:14:43.145172Z"
    }
   },
   "outputs": [],
   "source": [
    "Force_retraining = False\n",
    "image_location = '../face_images/consolidated_data/'\n",
    "if (os.path.isfile(os.path.join(image_location,'classifier.pickle')) & os.path.isfile(os.path.join(image_location,'label_encoder.pickle'))& (not Force_retraining)):\n",
    "    clf = pickle.load(open(os.path.join(image_location,'classifier.pickle'),'rb'))\n",
    "    L_enc = pickle.load(open(os.path.join(image_location,'label_encoder.pickle'),'rb'))\n",
    "else:\n",
    "    clf,L_enc = train_face_detection('../face_images/consolidated_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:17:50.715400Z",
     "start_time": "2019-09-10T07:17:50.713005Z"
    }
   },
   "outputs": [],
   "source": [
    "channel = '/home/prateek/Desktop/Video_Data/VIDEO_DATA_FOR_ANNOTATION/yt_person_data/london_3.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T07:18:46.791965Z",
     "start_time": "2019-09-10T07:17:50.987771Z"
    }
   },
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"Detection Results\",cv2.WINDOW_NORMAL)\n",
    "fd_thresh = .4\n",
    "# if labels:\n",
    "#     with open(labels, 'r') as f:\n",
    "#         labels_map = [x.strip() for x in f]\n",
    "# else:\n",
    "#     labels_map = None\n",
    "\n",
    "cap = cv2.VideoCapture(channel)\n",
    "retry_connect = 10\n",
    "cur_request_id = 0\n",
    "fps_fd = []\n",
    "net_fps = []\n",
    "while (cap.isOpened()):\n",
    "    fps_fd = fps_fd[-100:]\n",
    "    initial_w = cap.get(3)\n",
    "    initial_h = cap.get(4)\n",
    "    inf_start_fd = time.time()\n",
    "    ret,frame  = cap.read()\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(channel)\n",
    "        retry_connect-=1\n",
    "        if retry_connect<0:\n",
    "            break\n",
    "    frame = generate_plot(frame,clf,L_enc,thresh=.1)\n",
    "    det_time_fd = time.time()- inf_start_fd\n",
    "\n",
    "    fps_fd.append(1/det_time_fd)\n",
    "    cv2.putText(frame, \"Inference FPS Face detection: {:.3f} \".format(np.mean(fps_fd)), (10, int(initial_h - 50)), cv2.FONT_HERSHEY_COMPLEX, 0.5,\n",
    "            (10, 10, 200), 1)\n",
    "    net_fps.append(np.mean(fps_fd))\n",
    "    \n",
    "    #\n",
    "    render_start = time.time()\n",
    "    cv2.imshow(\"Detection Results\", frame)\n",
    "    render_end = time.time()\n",
    "    render_time = render_end - render_start\n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    if key == 27:\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T06:14:15.080548Z",
     "start_time": "2019-09-10T06:14:15.077440Z"
    }
   },
   "outputs": [],
   "source": [
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "openvino",
   "language": "python",
   "name": "openvino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
